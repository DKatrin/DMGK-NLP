{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af8893c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2041693509.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Katrin\\AppData\\Local\\Temp\\ipykernel_60532\\2041693509.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    def reversestring:\u001b[0m\n\u001b[1;37m                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def reversestring:\n",
    "    text=[::-1]\n",
    "    print(text)\n",
    "    \n",
    "def countletter:\n",
    "    count_capitalletter = 0\n",
    "    count_lowletter = 0\n",
    "    for i in text:\n",
    "        if i.isupper():\n",
    "            count_capitalletter+=1\n",
    "        if i.islower():\n",
    "            count_lowletter+=1\n",
    "    print(count_capitalletter, count_lowletter)\n",
    "        \n",
    "def palindrome:\n",
    "    if text == text[::-1]:\n",
    "        print (\"Palindrome\")\n",
    "    print (\"no palindrome\")\n",
    "    \n",
    "def sorthyphens:\n",
    "    List = []\n",
    "    List.sort()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16784686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no palindrome\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def palindrome(text):\n",
    "    if text == text[::-1]:\n",
    "        print (\"Palindrome\")\n",
    "    print (\"no palindrome\")\n",
    "print(palindrome(\"regal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82e26e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www.someurl.com\n"
     ]
    }
   ],
   "source": [
    "text = 'www.someurl.comwww'.replace('comwww', 'com').strip()\n",
    "print(text)\n",
    "\n",
    "with open('data/title.txt', 'r', encoding='UTF-8') as f:\n",
    "for line in f: \n",
    "    firstline = f.readline()\n",
    "t = f.readline(3).replace('European', 'Global')\n",
    "\n",
    "with open('data/title.txt', 'r', encoding='UTF-8') as f:\n",
    "    Liste = []\n",
    "    text = f.read().split('')\n",
    "    Liste.append(text)\n",
    "    for i in len(Liste[j]):\n",
    "        if Liste[j]==5:\n",
    "            print(Liste[j])\n",
    "        j+=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26081a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('data/treaty_of_lisbon.txt', 'r', encoding='UTF-8')\n",
    "for line in f:\n",
    "    Liste = []\n",
    "    f.readline()\n",
    "    if line == 'Article 8 a'\n",
    "        f.readline().Liste.append()\n",
    "        if line == 'Article 8 b'\n",
    "            break\n",
    "doc = load.nlp(Liste) #verben und nomenherausfiltern sowie deren frequency(->lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779273f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting entities for ner\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "with open ('data/am_i_the_asshole_example.text', 'r', encoding='UTF-8') as f:\n",
    "    text=f.read\n",
    "doc=nlp(text)\n",
    "for token in doc.ents:\n",
    "    print(token.text, token.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dde7b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment analysis\n",
    "!pip install spacytextblob\n",
    "!python -m textblob.download_corpora\n",
    "from spacytextblob.spacytextblob immport SpacyTextBlob\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "\n",
    "nlp.components #oder nlp.pipeline, gibt Funktionen aus, die man mit nlp machen kann\n",
    "nlp.add_pipe('spacytextblob')\n",
    "doc=nlp(text).replace('\\n', ' ')#?\n",
    "doc._.polarity\n",
    "for span in doc.sents: #satzweise Polarität ausgeben\n",
    "    blob=(TextBlob(span.text, analyzer=NaiveBayesAnalyzer())) # kann man weglassen, gibt aber ausführlichere Analyse aus\n",
    "    print(span.text, span._.polarity)\n",
    "    print('')\n",
    "for token in doc:\n",
    "    print(token, token._.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b852fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic modeling #gibt nur einen Trend aus, neue Abfrage kann zu leicht anderen Ergebnissen führen!\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "import numpy as np\n",
    "import os\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "path_to_folder= 'data/topic_models/'\n",
    "path_to_files= sorted([os.path.join(path_to_folder, f) for f in os.listdir(path_to_folder)])#createsalistofallthefilenamesinthefolder\n",
    "print(path_to_files)\n",
    "tokenized_corpus=[]\n",
    "extra_stop=['mr', 'mrs', 'ms', 'hon']\n",
    "for my_file in path_to_file:\n",
    "    with open (my_file, 'r', encoding='UTF-8') as f:\n",
    "        text = f.read().replace('\\n', '')\n",
    "        doc=nlp(text)\n",
    "        text_proc=[]\n",
    "        for token in doc: #corpus tokenizen\n",
    "            if not token.is_stop and not token.is_punct and not token.like_num and token.lemma_.lower() not in extra_stop:\n",
    "                #spacy hat eigene stopword-Liste\n",
    "                text_proc.append(token.lemma_.lower())\n",
    "        tokenized_corpus.append(text_proc)\n",
    "print(tokenized_corpus)\n",
    "words_id=corpora.Dictionary(tokenized_corpus)\n",
    "corpus=[words_id.doc2bow(txt) for txt in tokenized_corpus]\n",
    "k_init=5 #numeroftopics\n",
    "k_final=15\n",
    "for k in range (k_init, k_final+1):\n",
    "    lda_model=gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=words_id, num_topics=k, random_state=50, passes=20, per_word_topics=True)\n",
    "    per_lda=ld-model.log_perplexity(corpus)\n",
    "    coherence_model_lda=coherenceModel(model=lda_mode, texts=tokenized_corpus, dictionary=words_id, coherence='c_v')\n",
    "    coherence_lda=coherence_model_lda.get_coherence()\n",
    "    print(k, per_lda, coherence_lda) #numberoftopics, complexity, coherencescore werden ausgegeben #werdeninrelationzueinanderangegebenungleichsentimentanalysismitabsolutenwerten\n",
    "lda_model=gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=words_id, num_topics=14, random_state=50, passes=20, per_word_topics=True)\n",
    "    per_lda=ld-model.log_perplexity(corpus)\n",
    "lda_model.show_topis(num_words=20, num topics=14)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
